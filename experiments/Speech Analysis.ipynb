{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["Ftbtv5m7bR2t","xdidUaXrbYIf"],"authorship_tag":"ABX9TyOtWAN/b6Qd/05XkuaD7nLt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8a0c026cd75544dcac4154489d1f9599":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ce34a6e2dd34f1c9dc98b4c79676951","IPY_MODEL_f622180f45d44f7e95accc65281bcbd7","IPY_MODEL_9d154b750f62460e8c8657a6aedca533"],"layout":"IPY_MODEL_c7e2c04e855f4512bca1f0f2ac2a72df"}},"3ce34a6e2dd34f1c9dc98b4c79676951":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e1a631deec945babc60143b8e019507","placeholder":"​","style":"IPY_MODEL_9cc8292d32cf48ae8beca0ab214bc268","value":"Map: 100%"}},"f622180f45d44f7e95accc65281bcbd7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_acd5df4d94684f6982009f7a02eb572b","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b689f34ab30a4e0e9015d4fc8d3161e8","value":4}},"9d154b750f62460e8c8657a6aedca533":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa9fa80eb06c4ae9ba2db7304f002fd7","placeholder":"​","style":"IPY_MODEL_fecc06f143a74092b9cd8098d566183e","value":" 4/4 [00:01&lt;00:00,  2.43 examples/s]"}},"c7e2c04e855f4512bca1f0f2ac2a72df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e1a631deec945babc60143b8e019507":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cc8292d32cf48ae8beca0ab214bc268":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acd5df4d94684f6982009f7a02eb572b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b689f34ab30a4e0e9015d4fc8d3161e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa9fa80eb06c4ae9ba2db7304f002fd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fecc06f143a74092b9cd8098d566183e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["### **Imports**"],"metadata":{"id":"sZY7ylEeb0c1"}},{"cell_type":"code","source":["!python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T-9SPaSLKqVh","executionInfo":{"status":"ok","timestamp":1732602685375,"user_tz":-330,"elapsed":609,"user":{"displayName":"Vishvam Sundararajan","userId":"00924487098506574872"}},"outputId":"0086df27-9fd3-47d1-fc10-0e4ac6a60439"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.10.12\n"]}]},{"cell_type":"code","source":["# !pip install datasets"],"metadata":{"id":"WU4YDH1PWduk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchaudio\n","import librosa\n","import numpy as np\n","\n","from datasets import Dataset\n","from collections import Counter\n","from transformers import pipeline\n","\n","from concurrent.futures import ThreadPoolExecutor, as_completed"],"metadata":{"id":"xjXNjU4M88rA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Constants**"],"metadata":{"id":"g9SIMYrxb32X"}},{"cell_type":"code","source":["AUDIO_FILE = \"./harvard.wav\"\n","\n","SAMPLING_RATE = 20000\n","CHUNK_DURATION = 10 # in seconds\n","CHUNK_OVERLAP_DURATION = 2 # in seconds\n","\n","EMOTION_TOP_N_RESULTS = 3"],"metadata":{"id":"ic9r52xkWvOu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Preprocessing**"],"metadata":{"id":"Ftbtv5m7bR2t"}},{"cell_type":"code","source":["def create_chunks_with_padding(audio, sr, start, end, chunk_duration, step_size, min_size):\n","\n","    local_chunks = []\n","    for i in range(start, end, int(step_size * sr)):\n","        chunk_end = i + int(chunk_duration * sr)\n","        chunk = audio[i:chunk_end]\n","\n","        if len(chunk) < min_size:\n","            chunk = np.pad(chunk, (0, min_size - len(chunk)), mode=\"constant\")\n","\n","        local_chunks.append((chunk, i / sr, chunk_end / sr))\n","    return local_chunks\n","\n","def chunk_audio_parallel_with_padding(audio_path, chunk_duration=CHUNK_DURATION, overlap=CHUNK_OVERLAP_DURATION, sample_rate=SAMPLING_RATE, num_workers=4):\n","\n","    audio, sr = librosa.load(audio_path, sr=sample_rate, mono=True)\n","    step_size = chunk_duration - overlap\n","    min_size = int(chunk_duration * sr)\n","\n","    # Divide the audio range into sections for parallel processing\n","    total_length = len(audio)\n","    section_size = total_length // num_workers\n","    sections = [(audio, sr, i, min(i + section_size, total_length), chunk_duration, step_size, min_size)\n","                for i in range(0, total_length, section_size)]\n","\n","    # Process each section in parallel\n","    chunks = []\n","    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n","        results = executor.map(lambda args: create_chunks_with_padding(*args), sections)\n","        for result in results:\n","            chunks.extend(result)\n","\n","    return chunks"],"metadata":{"id":"crUPkWk9CfDg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Tone / Emotion Analysis**"],"metadata":{"id":"xdidUaXrbYIf"}},{"cell_type":"code","source":["pipe = pipeline(\n","    \"audio-classification\",\n","    model=\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\",\n","    device=0\n",")"],"metadata":{"id":"2wH8oXru8L07","executionInfo":{"status":"ok","timestamp":1732539970434,"user_tz":-330,"elapsed":2861,"user":{"displayName":"Vishvam Sundararajan","userId":"00924487098506574872"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7bd341dc-5fe2-43fe-d7fc-a14288e3fe3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n","- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["def process_chunk(chunk_data):\n","    chunk, start, end = chunk_data\n","    chunk_results = pipe(chunk)\n","\n","    print(\"Debug : \", chunk_results, \"\\n\\n\")\n","\n","    return {\n","        \"timestep\": f\"{start:.2f}-{end:.2f}\",\n","        \"emotion\": chunk_results[0][\"label\"],\n","        \"score\": chunk_results[0][\"score\"]\n","    }\n","\n","def analyze_chunks_parallel(chunks, max_workers=8):\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        results = list(executor.map(process_chunk, chunks))\n","    return results"],"metadata":{"id":"qhv2qdqYbf5o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Pitch and Modulation Analysis**"],"metadata":{"id":"bnZGiLzwbijv"}},{"cell_type":"code","source":["def extract_pitch(audio, sr):\n","    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sr)\n","    pitch = [np.max(pitches[:, t]) for t in range(pitches.shape[1])]\n","    return np.array(pitch)\n","\n","def extract_loudness(audio):\n","    rms = librosa.feature.rms(y=audio)\n","    return rms[0]\n","\n","def analyze_modulation_and_pitch(audio, sr):\n","    pitch = extract_pitch(audio, sr)\n","    loudness = extract_loudness(audio)\n","\n","    pitch_variation = np.std(pitch)\n","    loudness_variation = np.std(loudness)\n","\n","    return pitch, loudness, pitch_variation, loudness_variation"],"metadata":{"id":"vxHEXctIaSHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Feedback Generation**"],"metadata":{"id":"J6ouq1xYk8-u"}},{"cell_type":"code","source":["def generate_feedback(pitch, loudness, pitch_variation, loudness_variation, emotion):\n","    feedback = {}\n","\n","    if emotion == 'happy':\n","        feedback[\"modulation\"] = \"Great job! Your speech has excellent variation in volume and pitch. It conveys positivity and energy.\"\n","    elif emotion == 'anger':\n","        feedback[\"modulation\"] = \"Your speech has strong variation in pitch and volume, conveying intensity. Be mindful of the tone to avoid sounding overly aggressive.\"\n","    elif emotion == 'sad':\n","        feedback[\"modulation\"] = \"Your speech lacks variation, which is typical for a sad tone. Try to vary your pitch and loudness to convey more nuance.\"\n","    elif emotion == 'neutral':\n","        feedback[\"modulation\"] = \"Your speech is steady, which is good for neutral delivery, but varying pitch and loudness could make it more engaging.\"\n","\n","    if emotion == 'happy':\n","        if np.mean(pitch) < 180:\n","            feedback[\"pitch\"] = \"Your pitch could be a little higher to reflect more excitement. Try to increase your pitch to convey more energy.\"\n","        else:\n","            feedback[\"pitch\"] = \"Your pitch is in a great range for happy speech. Keep the energy high!\"\n","    elif emotion == 'anger':\n","        if np.mean(pitch) < 220:\n","            feedback[\"pitch\"] = \"Your pitch is on the lower side for an angry tone. Consider raising your pitch for more intensity and to emphasize anger.\"\n","        else:\n","            feedback[\"pitch\"] = \"Your pitch is strong and conveys anger well, but be careful not to sound too harsh.\"\n","    elif emotion == 'sad':\n","        if np.mean(pitch) > 150:\n","            feedback[\"pitch\"] = \"For a sad tone, your pitch is higher than expected. Try lowering your pitch slightly to better convey sorrow.\"\n","        else:\n","            feedback[\"pitch\"] = \"Your pitch matches the sad tone well. Keep it steady and low to maintain the emotional depth.\"\n","    elif emotion == 'neutral':\n","        feedback[\"pitch\"] = \"Your pitch is within a comfortable range. It's neutral, but adding more variation could enhance engagement.\"\n","\n","    return feedback"],"metadata":{"id":"pHHQGOCwk8Jq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Speech Analyzer**"],"metadata":{"id":"k5avAzADbvYv"}},{"cell_type":"code","source":["def classify_audio(examples):\n","    audio_list = [np.array(audio, dtype=np.float32) for audio in examples[\"audio\"]]\n","\n","    # Classify emotions for each chunk of audio\n","    results = pipe(audio_list)\n","\n","    emotions, scores = [], []\n","\n","    for res in results :\n","      t1, t2 = [], []\n","      for i in range(EMOTION_TOP_N_RESULTS) :\n","        t1.append(res[i][\"label\"])\n","        t2.append(res[i][\"score\"])\n","      emotions.append(t1)\n","      scores.append(t2)\n","\n","    # emotions = [res[0][\"label\"] for res in results]\n","    # scores = [res[0][\"score\"] for res in results]\n","\n","    # Initialize the results\n","    feedback_list = []\n","\n","    # Process each audio chunk\n","    for i, audio in enumerate(audio_list):\n","        # Extract pitch and loudness\n","        pitch, loudness, pitch_variation, loudness_variation = analyze_modulation_and_pitch(audio, SAMPLING_RATE)\n","\n","        feedback = generate_feedback(pitch, loudness, pitch_variation, loudness_variation, emotions[i])\n","\n","        feedback_list.append({\n","            \"timestep\": examples[\"timestep\"][i],\n","            \"emotion\": emotions[i],\n","            \"score\": scores[i],\n","            # \"pitch\": pitch.tolist(),\n","            # \"loudness\": loudness.tolist(),\n","            # \"feedback\": feedback\n","        })\n","\n","    return {\"results\": feedback_list}"],"metadata":{"id":"Jr7OiQBobuhk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunks = chunk_audio_parallel_with_padding(AUDIO_FILE, chunk_duration=CHUNK_DURATION, overlap=CHUNK_OVERLAP_DURATION)\n","data = [\n","    {\"audio\": np.array(chunk[0], dtype=np.float32), \"timestep\": chunk[1]}\n","    for chunk in chunks\n","]\n","dataset = Dataset.from_list(data)"],"metadata":{"id":"xiD8fV-CaSBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.map(classify_audio, batched=True, batch_size=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["8a0c026cd75544dcac4154489d1f9599","3ce34a6e2dd34f1c9dc98b4c79676951","f622180f45d44f7e95accc65281bcbd7","9d154b750f62460e8c8657a6aedca533","c7e2c04e855f4512bca1f0f2ac2a72df","3e1a631deec945babc60143b8e019507","9cc8292d32cf48ae8beca0ab214bc268","acd5df4d94684f6982009f7a02eb572b","b689f34ab30a4e0e9015d4fc8d3161e8","fa9fa80eb06c4ae9ba2db7304f002fd7","fecc06f143a74092b9cd8098d566183e"]},"id":"Vfk2XhPdbGam","executionInfo":{"status":"ok","timestamp":1732542024154,"user_tz":-330,"elapsed":6103,"user":{"displayName":"Vishvam Sundararajan","userId":"00924487098506574872"}},"outputId":"c9fedc54-58b9-4443-a57a-0b834e69f259"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a0c026cd75544dcac4154489d1f9599"}},"metadata":{}}]},{"cell_type":"markdown","source":["### **Just an idea : (Smoothing and N-Gram Like Analysis)**\n","\n","- Instead of taking the top emotion, take the top 2 or 3\n","- For every 2-3 timesteps (depending on the `CHUNK_DURATION`), <br>take the 2 most occurring emotion and assign to all of them (**smoothing basically**)\n","- Using this combination on emotions, make a judgement"],"metadata":{"id":"zm_HMOxUgpY5"}},{"cell_type":"code","source":["results = dataset.to_pandas()[\"results\"].to_list()"],"metadata":{"id":"0d30FtHlbHcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emotions = [res[\"emotion\"] for res in results]"],"metadata":{"id":"kqfl7Nx7hkRZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","\n","SMOOTHING_FACTOR = 3\n","\n","smoothed_emotions = []\n","\n","for i in range(0, len(emotions), SMOOTHING_FACTOR):\n","\n","    temp = emotions[i : i + SMOOTHING_FACTOR]\n","\n","    flat_temp = [emotion.tolist() if isinstance(emotion, np.ndarray) else emotion for emotion in temp]\n","\n","    flat_temp = [item for sublist in flat_temp for item in (sublist if isinstance(sublist, list) else [sublist])]\n","\n","    emotion_counts = Counter(flat_temp)\n","\n","    most_frequent_emotion = emotion_counts.most_common(2)\n","    smoothed_emotions.append(most_frequent_emotion)\n","\n","    print(i, flat_temp, \"Most frequent:\", most_frequent_emotion)\n","\n","print(\"Smoothed Emotions:\", smoothed_emotions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-d4XUWCmhSOd","executionInfo":{"status":"ok","timestamp":1732542357937,"user_tz":-330,"elapsed":546,"user":{"displayName":"Vishvam Sundararajan","userId":"00924487098506574872"}},"outputId":"9925b8c1-4313-447a-da0f-23ab2ed0f3e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 ['calm', 'disgust', 'happy', 'disgust', 'calm', 'neutral', 'disgust', 'neutral', 'angry'] Most frequent: [('disgust', 3), ('calm', 2)]\n","3 ['disgust', 'happy', 'surprised'] Most frequent: [('disgust', 1), ('happy', 1)]\n","Smoothed Emotions: [[('disgust', 3), ('calm', 2)], [('disgust', 1), ('happy', 1)]]\n"]}]}]}